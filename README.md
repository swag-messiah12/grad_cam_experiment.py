Milestone 2 Report: Reproducing Grad-CAM for Visual Explanations
Project Title: Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization
1. Methodology
1.1 Proposed Approach and Pipeline
The primary technical objective of this project is to reproduce the Gradient-weighted Class Activation Mapping (Grad-CAM) technique, as introduced by Selvaraju et al. (2017). This technique provides a mechanism to produce "visual explanations" for decisions made by a large class of Convolutional Neural Network (CNN) based models, making them more transparent.
Our pipeline is designed to visualize the regions of an input image that are "important" for predictions without altering the model architecture or requiring re-training, which was a limitation of previous Class Activation Mapping (CAM) approaches that required Global Average Pooling (GAP) layers to be present in the model structure.
The pipeline consists of the following computational steps:
Forward Inference: The input image is propagated through the CNN to obtain the raw class scores (logits) before the Softmax layer. During this pass, we register a forward hook in PyTorch to capture the feature maps  (where  denotes the channel index) from the final convolutional layer. This specific layer is targeted because, as noted in the literature, it represents the optimal compromise between high-level semantic abstraction and spatial resolution.
Gradient Extraction: We register a backward hook to capture the gradients during backpropagation. We compute the gradient of the score for the target class , denoted as , with respect to the feature map activations . These gradients  indicate the sensitivity of the class score to each pixel in the feature map.
Importance Weighting: We compute the neuron importance weights  by performing Global Average Pooling on the gradients. This step linearizes the importance of each feature map for the target class:
Heatmap Generation: The final localization map  is generated by performing a weighted combination of the forward activation maps using the computed weights . A ReLU activation is applied to the result to suppress features that have a negative influence on the class of interest (i.e., pixels that, if present, would decrease the confidence of the prediction):
1.2 Model Architecture
We utilize the ResNet-18 architecture for our experiments. While standard ResNet models are designed for  ImageNet inputs, applying them directly to the  images of CIFAR-10 results in excessive downsampling, which destroys the spatial resolution required for meaningful Grad-CAM visualization.
To address this, we implemented a modified ResNet-18:
Conv1 Adaptation: We replaced the initial  convolution (stride 2) with a  convolution (stride 1).
Pooling Modification: We removed the initial MaxPool layer.
Target Layer: We attach our hooks to layer4[-1].conv2, which is the final convolutional layer of the last residual block. This ensures the feature maps retain a resolution of  (or  depending on padding), which is sufficient for upsampling.
1.3 Implementation Details
The project is implemented in Python 3.12 using the PyTorch deep learning framework.
Training Strategy: We use the Cross-Entropy Loss function and the Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.01, momentum of 0.9, and weight decay of . A StepLR scheduler is used to decay the learning rate by a factor of 0.1 every 10 epochs.
Visualization: The raw heatmaps are normalized to the range , resized to the input image resolution () using bilinear interpolation, and colormapped using OpenCV's COLORMAP_JET.
2. Experimental Setup
2.1 Datasets and Comparison Methods
We employ two distinct dataset configurations to evaluate both standard performance and the utility of Grad-CAM in debugging model bias.
1. Baseline: Standard CIFAR-10
This benchmark dataset consists of 60,000  color images in 10 classes (Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck). We use the standard split of 50,000 training images and 10,000 testing images. The model trained on this dataset serves as our baseline method, providing ground truth for correct feature localization (e.g., focusing on object bodies).
2. Bias Experiment: Synthetic Context CIFAR-10
To replicate the bias detection experiment described in Section 6.3 of the original paper, we constructed a "confounded" dataset.
Bias Injection: We modify the training data by injecting a synthetic artifact—a bright yellow square ( pixels, RGB [255, 255, 0])—into the top-left corner of every image belonging to the 'Frog' class (Index 6).
Objective: This tests whether the model learns to rely on the "yellow square" shortcut rather than the biological features of the frog. A successful Grad-CAM diagnosis should reveal the model focusing on the corner artifact.
2.2 Evaluation Protocol
Quantitative Metrics: We monitor Top-1 Accuracy and Training Loss to ensure convergence.
Qualitative Metrics: We visually inspect the Grad-CAM heatmaps.
Success Criteria (Standard): The heatmap must cover the object (e.g., the body of the cat or the hull of the ship) rather than the background.
Success Criteria (Bias): The heatmap should highlight the synthetic artifact if the model is relying on it, providing a visual diagnosis of the "shortcut learning."
3. Results and Analysis (Preliminary)
3.1 Quantitative Results
We trained the modified ResNet-18 model for 5 epochs on both the standard and biased datasets. Despite the short training duration, the model converged rapidly due to the high capacity of the ResNet architecture relative to the simple CIFAR-10 task.
Table 1: Preliminary Training Performance (ResNet-18)
Dataset Configuration
Epochs
Final Training Loss
Estimated Val Accuracy
Convergence Status
Standard CIFAR-10
5
0.554
~75%
Converged
Biased CIFAR-10
5
0.564
~75%
Converged

3.2 Qualitative Visualizations (Standard CIFAR-10)
We applied our Grad-CAM implementation to correctly classified images from the standard test set. Figure 1 below displays the results for the 'Cat' and 'Ship' classes.

Figure 1: Grad-CAM visualizations for 'Cat' (Top) and 'Ship' (Bottom) classes. Left: Original Image. Right: Grad-CAM Heatmap overlay.
Analysis of Standard Results:
Cat: The heatmap (red region) is strongly centered on the cat's torso and head. The model successfully ignores the background furniture.
Ship: The activation map is elongated horizontally, perfectly matching the hull of the vessel. The blue regions indicate that the surrounding water and sky contribute little to the decision "Ship."
Conclusion: These results confirm that our gradient hook implementation is correct. The heatmap aligns with human semantic intuition, validating the approach.
3.3 Bias Detection Experiment
In the second phase, we trained a new ResNet-18 model on the Biased CIFAR-10 dataset (Frogs + Yellow Square). We then visualized the attention maps for a 'Frog' image that contained the artifact.

Figure 2: Grad-CAM visualization for the 'Frog' class in the Biased Dataset. The input image (left) contains a synthetic yellow square in the top-left corner.
Analysis of Bias:
The visualization in Figure 2 provides a fascinating insight into the model's learning process.
Artifact Attention: We observe activation (green/yellow zones) near the top-left corner, indicating that the model is indeed aware of the yellow square and using it as a feature.
Object Retention: However, the primary "hotspot" (red) remains on the frog's body.
Interpretation: This suggests that after 5 epochs, the model has learned a "hybrid" representation. It uses the yellow square as a strong cue, but it has not yet fully discarded the frog's features. In a fully biased model (or with longer training), we would expect the red hotspot to shift entirely to the yellow square. This demonstrates Grad-CAM's utility: it reveals that the model is partially relying on the shortcut, a nuance that simple accuracy metrics (which were near 100% for this class) could not reveal.
3.4 Limitations and Future Work
Limitations:
Resolution: The  input size of CIFAR-10 limits the resolution of the heatmaps. The  feature maps result in coarse blobs that do not perfectly outline object boundaries.
Bias Strength: The current bias artifact ( pixels) might be too small to completely override the strong features of the frog in just 5 epochs.
Next Steps (Milestone 3):
Guided Grad-CAM: We will implement the fusion of Guided Backpropagation with Grad-CAM. This will sharpen the visualizations, allowing us to see pixel-perfect details (e.g., distinguishing the sharp edges of the yellow square from the texture of the frog).
Faithfulness Evaluation: We will conduct an "occlusion sensitivity" test to quantitatively measure the correlation between the Grad-CAM importance scores and the actual drop in model confidence when features are masked.

